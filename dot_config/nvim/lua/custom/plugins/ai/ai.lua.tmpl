return {
  {
    "yetone/avante.nvim",
    event = "VeryLazy",
    version = false, -- Never set this value to "*"! Never!
    build = "make",
    dependencies = {
      "nvim-treesitter/nvim-treesitter",
      "stevearc/dressing.nvim",
      "nvim-lua/plenary.nvim",
      "MunifTanjim/nui.nvim",
      {
        -- Make sure to set this up properly if you have lazy=true
        'MeanderingProgrammer/render-markdown.nvim',
        opts = {
          file_types = { "markdown", "Avante" },
        },
        ft = { "markdown", "Avante" },
      },
    },
    opts = {
      openai = {
        endpoint = "https://ai.kth.pro/api/v1",
        model = "Instruct",  -- your desired model (or use gpt-4o, etc.)
        timeout = 30000,   -- Timeout in milliseconds, increase this for reasoning models
        temperature = 0.8,
        max_tokens = 8192, -- Increase this to include reasoning tokens (for reasoning models)
        -- reasoning_effort = "medium", -- low|medium|high, only used for reasoning models
      },
{{- if eq .email "g.kuzora@korona.net" }}
      provider = "koronatech",
      vendors = {
        koronatech = {
          __inherited_from = "openai",
          api_key_name = "KORONATECH_API_KEY",
          endpoint = "https://ai.kth.pro/api/v1",
          model = "Instruct",
        },
      },
{{- else }}
      provider = "mistral",
      vendors = {
        mistral= {
          __inherited_from = "openai",
          api_key_name = "MISTRAL_API_KEY",
          endpoint = "https://codestral.mistral.ai/v1",
          model = "codestral-latest",
          max_tokens = 4096, -- to avoid using max_completion_tokens
        },
      },
{{- end }}
      file_selector = {
        provider = "snacks",
        -- Options override for custom providers
        provider_opts = {},
      }
    },
  },
{{- if eq .email "g.kuzora@korona.net" }}
  -- {
  --   'milanglacier/minuet-ai.nvim',
  --   dependencies = {
  --     'nvim-lua/plenary.nvim',
  --   },
  --   keys = {
  --     { "<leader>av", '<cmd>Minuet virtualtext toggle<cr>', desc = "Toggle virtual text" },
  --   },
  --   config = function()
  --     require('minuet').setup {
  --       provider = 'openai_fim_compatible',
  --       n_completions = 1, -- recommended for local model for resource saving
  --       context_window = 512, -- adjust context window size for available computing resources
  --       provider_options = {
  --         openai_fim_compatible = {
  --           api_key = 'AIAPI',
  --           name = 'qwen',
  --           end_point = 'https://ai.kth.ru/api/v1/completion',
  --           model = 'Qwen2.5-Coder-32B-Instruct-AWQ',
  --           optional = {
  --             max_tokens = 256,
  --             top_p = 0.9,
  --           },
  --         },
  --       },
  --       virtualtext = {
  --         auto_trigger_ft = { 'python', 'lua', 'go' },
  --         show_on_completion_menu = true,
  --         keymap = {
  --           -- accept whole completion
  --           accept = '<C-A-y>',
  --           -- accept one line
  --           accept_line = '<C-y>',
  --           -- accept n lines (prompts for number)
  --           -- e.g. "A-z 2 CR" will accept 2 lines
  --           accept_n_lines = '<A-z>',
  --           -- Cycle to prev completion item, or manually invoke completion
  --           prev = '<A-[>',
  --           -- Cycle to next completion item, or manually invoke completion
  --           next = '<A-]>',
  --           dismiss = '<A-e>',
  --         },
  --       },
  --     }
  --     vim.cmd('Minuet virtualtext disable')
  --   end,
  -- },
{{- else }}
  {
    "supermaven-inc/supermaven-nvim",
    event = { "BufReadPre", "BufNewFile" },
    keys = {
      { "<leader>av", '<cmd>SupermavenToggle<cr>', desc = "supermaven: toggle" },
    },
    config = function()
      require("supermaven-nvim").setup {
        keymaps = {
          accept_suggestion = "<C-y>",
          clear_suggestion = "<C-]>",
          accept_word = "<C-j>",
        },
      }
    end,
  },
{{- end }}
  -- {
  -- 'olimorris/codecompanion.nvim',
  -- event = { 'BufReadPre', 'BufNewFile' },
  -- dependencies = {
  --   'nvim-lua/plenary.nvim',
  --   'nvim-treesitter/nvim-treesitter',
  --   {
  --     'echasnovski/mini.diff',
  --     version = false,
  --     config = function()
  --       require('mini.diff').setup()
  --     end,
  --   },
  -- },
  -- opts = {
  --   adapters = {
  --     -- opts = {
  --     --   allow_insecure = true,
  --     -- },
  --     cftai = function()
  --     return require("codecompanion.adapters").extend("openai_compatible", {
  --       env = {
  --         url = "https://ai.kth.ru/api", -- optional: default value is ollama url http://127.0.0.1:11434
  --         api_key = "", -- optional: if your endpoint is authenticated
  --         chat_url = "/v1/chat/completions", -- optional: default value, override if different
  --         models_endpoint = "/v1/models", -- optional: attaches to the end of the URL to form the endpoint to retrieve models
  --         model = "schema.model.default",
  --       },
  --       schema = {
  --         model = {
  --           default = "Qwen2.5-Coder-32B-Instruct-AWQ",  -- define llm model to be used
  --         },
  --         -- temperature = {
  --         --   order = 2,
  --         --   mapping = "parameters",
  --         --   type = "number",
  --         --   optional = true,
  --         --   default = 0.8,
  --         --   desc = "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.",
  --         --   validate = function(n)
  --         --     return n >= 0 and n <= 2, "Must be between 0 and 2"
  --         --   end,
  --         -- },
  --         -- max_completion_tokens = {
  --         --   order = 3,
  --         --   mapping = "parameters",
  --         --   type = "integer",
  --         --   optional = true,
  --         --   default = nil,
  --         --   desc = "An upper bound for the number of tokens that can be generated for a completion.",
  --         --   validate = function(n)
  --         --     return n > 0, "Must be greater than 0"
  --         --   end,
  --         -- },
  --         -- stop = {
  --         --   order = 4,
  --         --   mapping = "parameters",
  --         --   type = "string",
  --         --   optional = true,
  --         --   default = nil,
  --         --   desc = "Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.",
  --         --   validate = function(s)
  --         --     return s:len() > 0, "Cannot be an empty string"
  --         --   end,
  --         -- },
  --         -- logit_bias = {
  --         --   order = 5,
  --         --   mapping = "parameters",
  --         --   type = "map",
  --         --   optional = true,
  --         --   default = nil,
  --         --   desc = "Modify the likelihood of specified tokens appearing in the completion. Maps tokens (specified by their token ID) to an associated bias value from -100 to 100. Use https://platform.openai.com/tokenizer to find token IDs.",
  --         --   subtype_key = {
  --         --     type = "integer",
  --         --   },
  --         --   subtype = {
  --         --     type = "integer",
  --         --     validate = function(n)
  --         --       return n >= -100 and n <= 100, "Must be between -100 and 100"
  --         --     end,
  --         --   },
  --         -- },
  --       },
  --     })
  --   end,
  --   },
  --   strategies = {
  --     chat = {
  --       adapter = 'cftai',
  --       keymaps = {
  --         completion = {
  --           modes = {
  --             i = '<C-a>',
  --           },
  --           index = 1,
  --           callback = 'keymaps.completion',
  --           description = 'Completion Menu',
  --         },
  --       },
  --     },
  --     inline = {
  --       adapter = 'cftai',
  --     },
  --     cmd = {
  --       adapter = 'cftai',
  --     },
  --   },
  --   display = {
  --     diff = {
  --       -- enabled = true,
  --       -- close_chat_at = 240, -- Close an open chat buffer if the total columns of your display are less than...
  --       -- layout = "vertical", -- vertical|horizontal split for default provider
  --       -- opts = { "internal", "filler", "closeoff", "algorithm:patience", "followwrap", "linematch:120" },
  --       provider = 'mini_diff', -- default|mini_diff
  --     },
  --   },
  -- },
  -- keys = {
  --   {
  --     '<leader>at',
  --     '<cmd>CodeCompanionChat Toggle<cr>',
  --     desc = 'Toggle CodeCompanionChat',
  --     mode = { 'n', 'v', 'x' },
  --   },
  --   {
  --     '<leader>an',
  --     '<cmd>CodeCompanionChat<cr>',
  --     desc = 'New CodeCompanionChat',
  --     mode = { 'n', 'v', 'x' },
  --   },
  --   {
  --     '<leader>ai',
  --     '<cmd>CodeCompanion<cr>',
  --     desc = 'Inline CodeCompanionChat',
  --     mode = { 'n', 'v', 'x' },
  --   },
  --   {
  --     '<leader>aa',
  --     '<cmd>CodeCompanionChat Add<cr>',
  --     desc = 'Add to CodeCompanionChat',
  --     mode = { 'n', 'v', 'x' },
  --   },
  --   {
  --     '<leader>as',
  --     '<cmd>CodeCompanionActions<cr>',
  --     desc = 'Actions CodeCompanionChat',
  --     mode = { 'n', 'v', 'x' },
  --   },
  -- },
  -- config = true,
  -- },
}
